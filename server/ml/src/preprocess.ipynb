{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "### Functions:\n",
    "\n",
    "Beside the function for each step, there are 2 pipline functions:\n",
    "\n",
    "* dataset_pipline - Running the pipeline for a whole dataset (useful for processing text before training/testing)\n",
    "* text_pipeline - Running the pipeline for a single text document (useful for processing text before prediction)\n",
    "\n",
    "### These are all the functions required to preprocess the data\n",
    "The data pipeline consists of the following steps:\n",
    "1. Removing rows with null from the original dataset\n",
    "2. Transforming the text to lowercase\n",
    "3. Removing spaces, newlines and tabs\n",
    "4. Tokenizing the text to an array of words\n",
    "5. Removing stopwords from the text (using nltk english stopwords)\n",
    "6. Lemmatizing the text (using WordNet Lemmatizer)\n",
    "7. Saving the dataset to a new file by a given name\n",
    "    * The saving is only for dataset_pipeline function\n",
    "8. Resetting the tokens and returning the processed text to a string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from os import path\n",
    "\n",
    "BASE_DATASET_PATH = \"..\\\\datasets\"\n",
    "TRAIN_DATASET_PATH = path.join(BASE_DATASET_PATH,\"train.csv\")\n",
    "TEST_DATASET_PATH = path.join(BASE_DATASET_PATH,\"test.csv\")\n",
    "DATA_COLUMNS = [\"comment_text\"]\n",
    "LABEL_COLUMNS = [\"toxic\",'severe_toxic','obscene','threat','insult','identity_hate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "test_dataset = pd.read_csv(TEST_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nulls(dataset):\n",
    "    print(\"Dropping nulls...\")\n",
    "    print(\"Dataset size before - \"+ str(len(dataset)))\n",
    "    dataset = dataset.dropna()\n",
    "    print(\"Dataset size after - \"+ str(len(dataset)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text\n",
       "0  Explanation\\nWhy the edits made under my usern...\n",
       "1  D'aww! He matches this background colour I'm s...\n",
       "2  Hey man, I'm really not trying to edit war. It...\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4  You, sir, are my hero. Any chance you remember..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_dataset[DATA_COLUMNS]\n",
    "y_train = train_dataset[LABEL_COLUMNS]\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lowecase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text):\n",
    "    text = ' '.join(text.split())\n",
    "    return re.sub(r'\\s{2,}',\" \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    # return every character in the text if it is not punctuation\n",
    "    # To rebuild the sentece, we will join the characters in the list without any seperator\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return re.sub(r'\\s{2,}',\" \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokenized = text.split(' ')\n",
    "    return [word for word in tokenized if word != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(tokenized_text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [word for word in tokenized_text if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(tokenized_text):\n",
    "    return [wn.lemmatize(word) for word in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tokens(tokenize_text):\n",
    "    return \" \".join(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(filename,before,after):\n",
    "    before[DATA_COLUMNS] = after\n",
    "    before = before.reset_index(drop=True)\n",
    "    before.to_csv(path.join(BASE_DATASET_PATH,filename),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping nulls...\n",
      "Dataset size before - 159571\n",
      "Dataset size after - 159571\n",
      "Transforming to lowercase...\n",
      "Removing spaces...\n",
      "Removing punctuations...\n",
      "Tokenizing text...\n",
      "Removing stopwords...\n",
      "Lemmatizing text...\n",
      "Resetting tokens...\n",
      "Saving to new file train_processed.csv\n",
      "Dropping nulls...\n",
      "Dataset size before - 153164\n",
      "Dataset size after - 153164\n",
      "Transforming to lowercase...\n",
      "Removing spaces...\n",
      "Removing punctuations...\n",
      "Tokenizing text...\n",
      "Removing stopwords...\n",
      "Lemmatizing text...\n",
      "Resetting tokens...\n",
      "Saving to new file test_processed.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>yo bitch ja rule succesful youll ever whats ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>rfc title fine imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>source zawe ashton lapland â€”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>look back source information updated correct f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>dont anonymously edit article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>totally agree stuff nothing toolongcrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>throw field home plate get faster throwing cut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>okinotorishima category see change agree corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>one founding nation eu germany law return quit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>stop already bullshit welcome im fool think ki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text\n",
       "0       00001cee341fdb12  yo bitch ja rule succesful youll ever whats ha...\n",
       "1       0000247867823ef7                                 rfc title fine imo\n",
       "2       00013b17ad220c46                       source zawe ashton lapland â€”\n",
       "3       00017563c3f7919a  look back source information updated correct f...\n",
       "4       00017695ad8997eb                      dont anonymously edit article\n",
       "...                  ...                                                ...\n",
       "153159  fffcd0960ee309b5            totally agree stuff nothing toolongcrap\n",
       "153160  fffd7a9a6eb32c16  throw field home plate get faster throwing cut...\n",
       "153161  fffda9e8d6fafa9e  okinotorishima category see change agree corre...\n",
       "153162  fffe8f1340a79fc2  one founding nation eu germany law return quit...\n",
       "153163  ffffce3fb183ee80  stop already bullshit welcome im fool think ki...\n",
       "\n",
       "[153164 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_pipeline(dataset,filename):\n",
    "    dataset = remove_nulls(dataset)\n",
    "    xs = dataset[DATA_COLUMNS]\n",
    "    print(\"Transforming to lowercase...\")\n",
    "    xs = xs.applymap(lambda x: transform_lowecase(x))\n",
    "    print(\"Removing spaces...\")\n",
    "    xs = xs.applymap(lambda x: remove_spaces(x))\n",
    "    print(\"Removing punctuations...\")\n",
    "    xs = xs.applymap(lambda x: remove_punctuations(x))\n",
    "    print(\"Tokenizing text...\")\n",
    "    xs = xs.applymap(lambda x: tokenize_text(x))\n",
    "    print(\"Removing stopwords...\")\n",
    "    xs = xs.applymap(lambda x: remove_stopwords(x))\n",
    "    print(\"Lemmatizing text...\")\n",
    "    xs = xs.applymap(lambda x: lemmatize_text(x))\n",
    "    print(\"Resetting tokens...\")\n",
    "    xs = xs.applymap(lambda x: reset_tokens(x))\n",
    "    print(\"Saving to new file \"+filename)\n",
    "    save_processed_data(filename,dataset,xs)\n",
    "    return dataset\n",
    "\n",
    "dataset_pipeline(train_dataset,\"train_processed.csv\")\n",
    "dataset_pipeline(test_dataset,\"test_processed.csv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey going show awesome text'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_pipeline(text):\n",
    "    text = transform_lowecase(text)\n",
    "    text = remove_spaces(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = tokenize_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = reset_tokens(text)\n",
    "    return text\n",
    "text_pipeline(\"Hey,   \\n I am going to show you my   awesome text!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43f2e1adb01b6c00601f1e13068a3e8015dfa9668196630167adc397fda2011b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
